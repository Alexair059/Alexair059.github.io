# 朴素贝叶斯

朴素贝叶斯（Naive Bayes）法是基于贝叶斯定理和特征条件独立假设的分类方法，实现简单，学习与预测的效率都很高，是一种常用的生成学习方法

## 分类

- 监督学习方法
- 概率模型
- 参数化模型
- 贝叶斯学习
- 生成学习方法

## 基本方法

设训练数据集为

$$
T=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\},\quad x\in \mathcal{X},y\in \mathcal{Y}
$$

其中输入空间 $\mathcal{X}\subseteq R^n$ 为 $n$ 维空间向量的集合，输出空间为类标记集合 $\mathcal{Y}=\{c_1,c_2,···,c_K\}$

令 $X$ 和 $Y$ 分别为定义在输入、输出空间上的随机变量，朴素贝叶斯法基于贝叶斯定理首先将最终目标，即条件概率分布 $P(Y=c_k \mid X=x)$ 转化为求解联合概率分布 $P(X=x,Y=c_k)$

$$
\begin{aligned}
P(Y=c_k \mid X = x) &= \frac{P(X=x,Y=c_k)}{P(X=x)}\\
&=\frac{P(Y=c_k)P(X=x \mid Y=c_k)}{\sum_{Y}P(Y)P(X = x \mid Y)}
\end{aligned}
$$

为此，朴素贝叶斯需要首先学习到先验概率

$$
P(Y=c_k),\quad k=1,2,···,K
$$

以及相应条件概率分布

$$
\begin{aligned}
P(X=x \mid Y=c_k)=&(X^{(1)}=x^{(1)},···,X^{(n)}=x^{(n)}\mid Y=c_k)\\
&k=1,2,···,K
\end{aligned}
$$

问题在于朴素贝叶斯对于条件概率分布 $P(X=x \mid Y=c_k)$ 的估计，在实际的样本中该分布有着指数级数量的参数需要估计

设 $X^{(i)}$ 可取值有 $S_i$ 个，$i=1,2,···,n$ ，$Y$ 的取值有 $K$ 个，则所需学习参数个数为 $K\prod_{i=1}^{n}S_i$

为此朴素贝叶斯对条件概率作了一个较强的条件独立性假设：

$$
\begin{aligned}
P(X=x \mid Y=c_k)&=(X^{(1)}=x^{(1)},···,X^{(n)}=x^{(n)}\mid Y=c_k)\\
&=\prod_{i=1}^{n}P(X^{(i)}=x^{(i)} \mid Y=c_k)
\end{aligned}
$$

具体而言，该假设认为用于分类的特征在类确定的前提下是条件独立的。这一假设使参数学习变得简单（参数个数变为了 $K\sum_{i=1}^{n}S_i$ ），但会牺牲一定的准确率

于是，在学习到参数模型后，朴素贝叶斯分类器可表示为：

$$
y = f(x) = \mathop{\arg\max}\limits_{c_k}\frac{P(Y=c_k)\prod_{i}P(X^{(i)}=x^{(i)} \mid Y=c_k)}{\sum_{k}P(Y)\prod_{i}P(X^{(i)} = x^{(i)} \mid Y=c_k)}
$$

因为对所有 $c_k$ 上式分母都相同，所以只需：

$$
y = f(x) = \mathop{\arg\max}\limits_{c_k}P(Y=c_k)\prod_{i}P(X^{(i)}=x^{(i)} \mid Y=c_k)
$$

## 参考资料

-   《统计学习方法（第二版）》李航