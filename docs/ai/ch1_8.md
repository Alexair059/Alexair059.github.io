---
comments: true
---

# PCA

主成分分析（PCA，principal component analysis）是应用广泛的无监督学习方法，通过将高维数据投射到最大化方差分布的主成分（PC，principal component）方向上，以获得新的信息更丰富、彼此更独立的新特征，常用于数据降维

## 分类

- 无监督学习方法
- 非概率模型
- 参数化模型

## 基本形式

假设样本写作列向量，即

$$
\mathbf{x} = [x^{(1)},x^{(2)},···,x^{(p)}]^\mathrm{T}
$$

PCA 寻求 $k$ 个主成分向量，将原样本在其上的投影作为降维后新的数据特征，通常 $k<p$

$$
\mathbf{u} = [u^{(1)},u^{(2)},···,u^{(k)}]^\mathrm{T}\\
$$

每个 $u^{(i)}$ 的坐标由中心化后的原样本投射到主成分上的数值所决定，可以视为原特征项的线性加权

$$
\begin{aligned}
u^{(i)} &= \mathbf{x}^\mathrm{T}\mathbf{v}_i\\
\mathbf{u} &= [\mathbf{v}_1,\mathbf{v}_2,···,\mathbf{v}_k]^\mathrm{T}\mathbf{x}
\end{aligned}
$$

其中每个主成分向量为

$$
\mathbf{v} = [v^{(1)},v^{(2)},···,v^{(p)}]^\mathrm{T}
$$

便是我们寻求的参数

## 损失策略

假设现有 $n$ 个 $p$ 维原始样本

$$
\mathbf{x}_i = [x_i^{(1)},x_i^{(2)},···,x_i^{(p)}]^\mathrm{T},\quad i=1,2,···,n
$$

PCA 得到 $n$ 个 $k$ 维原始样本

$$
\mathbf{u}_i = [u_i^{(1)},u_i^{(2)},···,u_i^{(k)}]^\mathrm{T},\quad i=1,2,···,n
$$

考虑最终目标：最大化投影到主成分的**样本**分布方差，我们分析其中一个主成分投影的情形（即我们研究所有的 $u_i^{(1)}$），其余类似

$$
\begin{aligned}
Var\left(u^{(1)}\right) &= \sum_{i=1}^n\left(u_i^{(1)}-\overline{u^{(1)}}\right)^2P(u^{(1)}=u_i^{(1)})\\
&= \frac{1}{n}\sum_{i=1}^n\left(u_i^{(1)}-\overline{u^{(1)}}\right)^2
\end{aligned}
$$

进一步简化，我们可以使 $\overline{u^{(1)}} = 0$，并省略掉系数

$$
L = \sum_{i=1}^n\left(u_i^{(1)}\right)^2
$$

对后面 $k$ 个主成分投影得到的特征项 $u^{(i)}$，我们都可以写出类似的目标损失

## 优化学习

有了目标损失，我们需要将其与参数联系起来，并制定学习方法。好在 PCA 不需要依赖梯度下降，经过推导可以发现它与特征值的关系，从而一步到位

### 如何投影

对于两个向量，其中一个到另一个的投影在一定条件下可以写作简洁的内积形式。而如果我们实现对原数据进行中心化，投影在数值上便可以认为就是在主成分方向上的新坐标，即 $u^{(i)}$

$$
u_i^{(1)} = \mathbf{x}_i^\mathrm{T}\mathbf{v}_1， \quad \Vert \mathbf{v}_1\Vert = 1
$$

其中第一个主成分 $\mathbf{v}_1$ 即是我们所需求的参数，注意此时还需有限制条件，即 $\mathbf{v}_1$ 的模为 1

因此我们有

$$
\begin{aligned}
L(\mathbf{v}_1) &= \sum_{i=1}^n(\mathbf{x}_i^\mathrm{T}\mathbf{v}_1)^2\\
&= [\mathbf{x}_1^\mathrm{T}\mathbf{v}_1,\mathbf{x}_2^\mathrm{T}\mathbf{v}_1,···,\mathbf{x}_n^\mathrm{T}\mathbf{v}_1][\mathbf{x}_1^\mathrm{T}\mathbf{v}_1,\mathbf{x}_2^\mathrm{T}\mathbf{v}_1,···,\mathbf{x}_n^\mathrm{T}\mathbf{v}_1]^\mathrm{T}\\
&= \mathbf{v}_1^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{X}\mathbf{v}_1
\end{aligned}
$$

其中，$\mathbf{X} = [\mathbf{x}_1,\mathbf{x}_2,···,\mathbf{x}_n]^\mathrm{T}$，$\mathbf{v}_1^\mathrm{T}\mathbf{v}_1 = 1$

于是终于能清楚表示

**PCA 问题**

$$
\begin{aligned}
&\max \mathbf{v}_1^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{X}\mathbf{v}_1\\
&s.t. \quad \mathbf{v}_1^\mathrm{T}\mathbf{v}_1 = 1
\end{aligned}
$$

### 特征值的关系

对于优化目标 $L(\mathbf{v}_1)$，如果我们令其值为 $\lambda$

$$
\begin{aligned}
\mathbf{v}_1^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{X}\mathbf{v}_1 &= \lambda = \lambda\mathbf{v}_1^\mathrm{T}\mathbf{v}_1\\
\Downarrow\\
\mathbf{v}_1^\mathrm{T}(\mathbf{X}^\mathrm{T}\mathbf{X}\mathbf{v}_1&-\lambda\mathbf{v}_1) = 0\\
\Downarrow\\
\mathbf{X}^\mathrm{T}\mathbf{X}\mathbf{v}_1&-\lambda\mathbf{v}_1 = 0
\end{aligned}
$$

即 $\mathbf{X}^\mathrm{T}\mathbf{X}\mathbf{v}_1 = \lambda\mathbf{v}_1$，这正好是矩阵特征向量及其特征值的定义，并且 $\lambda$ 就是我们的优化目标，于是问题转化为求解矩阵 $\mathbf{X}^\mathrm{T}\mathbf{X}$ 的特征向量与特征值

随着特征值由大到小，其对应的特征向量表示的主成分（需要单位化，因为得到的原始特征向量只是必要条件）投影下的样本分布方差逐渐减小，我们可以视情况选择前几个最大的特征值及其对应主成分

## 局限

优化过程中主要依靠特征值的大小选取主成分，如果特征值一样，将难以选取

最大化降维分布方差的策略不一定与最优化分类效果一致，可能寻找的主成分降维样本依旧难以分类