# 感知机

感知机（perceptron）是二分类的线性判别模型，其旨在求出将训练数据所处的特征空间进行线性划分的分离超平面。感知机学习算法简单，易于实现，具有原始形式与对偶形式，是神经网络与支持向量机的基础

## 分类
- 监督学习方法
- 非概率模型
- 参数化模型
- 线性模型
- 判别学习方法
- 二分类方法

## 感知机模型

设训练数据集为

$$
T=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\},\quad x\in \mathcal{X},y\in \mathcal{Y}
$$

其中输入空间 $\mathcal{X}\subseteq R^n$ 为 $n$ 维空间向量的集合，输出空间为类标记集合 $\mathcal{Y}=\{+1,-1\}$

由输入空间到输出空间的如下函数称为感知机：

$$
f(x)=sgn(w \cdot x +b)
$$

其中感知机参数 $w\in R^n$ 称为权值/权重， $b\in R$ 称作偏置

## 数据集的线性可分性

如果对于一个数据集 $T=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}$，如果存在某个超平面 $S:w \cdot x +b=0$ ,使得所有 $y_i=+1$ 的实例 $i$ ，有 $w\cdot x_i +b > 0$ （反之亦然），则称该数据集为线性可分的

## 感知机学习策略

假设训练数据集是线性可分的，感知机学习的目标即是求得一个能够将训练集正负实例点完全正确分开的分离超平面，即感知机参数 $w,b$

为此需要定义（经验）损失函数并将损失函数极小化

损失函数的一个直观选择即是误分类点的总数，但这样的损失函数不是参数 $w,b$ 的连续可导函数，不易优化，因此我们选择误分类点到超平面 $S$ 的总距离

对于任一误分类点 $x_0$ 到超平面的距离：

$$
\frac{1}{\Vert w\Vert _2}\vert w \cdot x_0 + b\vert
$$

对于误分类点 $(x_i,y_i)$ ,有

$$
-y_i(w \cdot x_i +b) > 0
$$

故所有误分类点到超平面 $S$ 的总距离为：

$$
-\frac{1}{\Vert w\Vert}\sum_{x_i \in M}y_i(w \cdot x_i +b)
$$

于是我们将感知机学习的损失函数定义为：

$$
L(w,b) = - \sum_{x_i \in M}y_i(w \cdot x_i +b)
$$

其中 $M$ 为误分类点的集合，$y_i(w \cdot x_i +b)$ 称为样本点的函数间隔

可以看出，损失函数 $L(w,b)$ 是非负的，对于一个特定的样本点，在误分类时是参数 $w,b$ 的线性函数，在正确分类时是0。因此，$L(w,b)$ 是 $w,b$ 的连续可导函数

## 感知机学习算法

感知机的学习算法采用随机梯度下降实现（SGD）：

首先随机初始化感知机参数 $w_0,b_0$ ，然后用梯度下降法不断极小化目标函数

$$
L(w,b)=-\sum_{x_i \in M}y_i(w \cdot x_i +b)
$$

在极小化过程中，不是一次使 $M$ 的所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降，其中 $L(w,b)$ 的梯度为

$$
\nabla_{w}L(w,b)=-\sum_{x_i \in M}y_ix_i \\
\nabla_{b}L(w,b)=-\sum_{x_i \in M}y_i
$$

随机选取一个误分类点 $(x_i,y_i)$ ，对参数进行更新

$$
w\gets w + \eta y_ix_i \\
b \gets b + \eta y_i
$$

其中 $\eta$ 称为学习率（learning rate）

