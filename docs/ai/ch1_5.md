---
comments: true
---

# 逻辑回归

逻辑回归（logistic regression）是概率统计视角下很经典的分类模型，将样本的特征与标签都视为随机变量，通过寻求最大化后验概率（MAP，maximum a posterior probability）来获得可分类的模型

## 分类

- 监督学习方法
- 概率模型
- 参数化模型
- 判别学习方法

## 基本形式

假设样本写作增广后的列向量，即

$$
\mathbf{x} = [1,x^{(1)},x^{(2)},···,x^{(p)}]^\mathrm{T}
$$

第一个对应参数的偏置项，剩下的即是样本的特征项

逻辑回归不直接输出实数，而是关注一个后验概率 $P(Y\mid X=\mathbf{x})$，在二分类的情形下，将标签分别编码为 0，1，我们可以具体假设该后验概率为 $Y=1$ 的概率

$$
P(Y=1\mid X=\mathbf{x}) = f(\mathbf{x};\boldsymbol{\theta})
$$

于是由对立事件可知，我们只需要关注该输出是否大于 0.5 即可，大于则为 $Y=1$，小于则为 $Y=-1$

但概率的取值是 $[0,1]$，我们该如何定义 $f(\mathbf{x};\boldsymbol{\theta})$ 呢？逻辑回归给出一个漂亮的式子

$$
\ln \left[\frac{P(Y=1\mid X=\mathbf{x})}{1-P(Y=1\mid X=\mathbf{x})}\right] = \mathbf{x}^\mathrm{T}\boldsymbol{\theta}
$$

如此便将后验概率取值映射到实数范围，于是有

$$
P(Y=1\mid X=\mathbf{x}) = f(\mathbf{x};\boldsymbol{\theta}) = \frac{e^{\mathbf{x}^\mathrm{T}\boldsymbol{\theta}}}{1+e^{\mathbf{x}^\mathrm{T}\boldsymbol{\theta}}}
$$

我们便得到了逻辑回归的基本形式

## 损失策略

利用极大似然估计（MLE，maximum likelihood estimation），我们将最大化对数似然函数作为优化的损失策略

有对数似然函数

$$
\begin{aligned}
L(\boldsymbol{\theta}) &= \ln \left[\prod_{i=1}^nP(Y=y_i\mid X=\mathbf{x_i})\right]\\
&= \sum_{i=1}^n\ln P(Y=y_i\mid X=\mathbf{x_i})\\
&= \sum_{i=1}^n\left[y_i\ln P(Y=1\mid X=\mathbf{x_i})+(1-y_i)\ln P(Y=0\mid X=\mathbf{x_i})\right]\\
&= \sum_{i=1}^n\left[y_i\ln \frac{e^{\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}}}{1+e^{\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}}}+(1-y_i)\ln \frac{1}{1+e^{\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}}}\right]\\
&= \sum_{i=1}^n\left[y_i\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}-\ln (1+e^{\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}})\right]
\end{aligned}
$$

令 $J(\boldsymbol{\theta}) = -L(\boldsymbol{\theta})$，作为损失函数，最大化 $L(\boldsymbol{\theta})$ 等价于最小化 $J(\boldsymbol{\theta})$

## 优化学习

常用方法为梯度下降与拟牛顿法

### 梯度下降

对损失函数 $J(\boldsymbol{\theta})$ 求一阶导

$$
\begin{aligned}
\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}) &= -\sum_{i}^n\left[y_i\mathbf{x}_i-\frac{\mathbf{x}_ie^{\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}}}{1+e^{\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}}}\right]\\
&= \sum_{i}^n\left[\frac{e^{\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}}}{1+e^{\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}}}-y_i\right]\mathbf{x}_i
\end{aligned}
$$

故有

$$
\begin{aligned}
\boldsymbol{\theta}^{t+1} &= \boldsymbol{\theta}^{t} - \alpha \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^{t})\\
\boldsymbol{\theta}^{t+1} &= \boldsymbol{\theta}^{t} - \alpha \sum_{i}^n\left[\frac{e^{\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}^{t}}}{1+e^{\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}^{t}}}-y_i\right]\mathbf{x}_i
\end{aligned}
$$