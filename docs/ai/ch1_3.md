---
comments: true
---

# 线性回归

线性回归（linear regression）是应用最为广泛的回归模型，但其基本思想与形式早已超出了单纯的回归场景。因为其推导形式比较简单、经典，就拿来记录一下，顺带结合相关的优化学习与正则技巧一起了

## 分类
- 监督学习方法
- 非概率模型
- 参数化模型

## 基本形式

假设样本写作增广后的列向量，即

$$
\boldmath{x} = [1,x^{(1)},x^{(2)},···,x^{(p)}]^\mathrm{T}
$$

即第一个对应参数的偏置项，剩下的即是样本的特征项，线性回归的模型相当简洁

$$
y = \theta_0+\theta_1x^{(1)}+\theta_2x^{(2)}+···\theta_px^{(p)}
$$

即

$$
y = \boldmath{x}^\mathrm{T}\boldmath{\theta}
$$

将整个样本写作矩阵形式，有

$$
\begin{aligned}
\boldmath{X} &= [\boldmath{x_1},\boldmath{x_2},···,\boldmath{x_n}]^\mathrm{T}\\
\boldmath{y} &= [y_1,y_2,···,y_n]^\mathrm{T}\\
\boldmath{\theta} &= [\theta_0,\theta_1,···,\theta_p]^\mathrm{T}
\end{aligned}
$$

则

$$
\boldmath{y} = f(\boldmath{X};\boldmath{\theta}) = \boldmath{X}\boldmath{\theta}
$$

## 损失策略

采取 SSE（sum of squared error），就是常见的平方损失，该函数是凸函数，这一优秀的性质是正规方程求最优解的基础

$$
\begin{aligned}
J(\boldmath{\theta}) &= \frac{1}{2}\sum_{i=1}^{n}(\hat{y_i}-y_i)^2\\
% &= \frac{1}{2}\sum_{i=1}^{n}(\boldmath{x}_i^\mathrm{T}\boldmath{\theta}-y_i)^2\\
&= \frac{1}{2}(\hat{\boldmath{y}}-\boldmath{y})^\mathrm{T}(\hat{\boldmath{y}}-\boldmath{y})\\
&= \frac{1}{2}(\boldmath{X}\boldmath{\theta}-\boldmath{y})^\mathrm{T}(\boldmath{X}\boldmath{\theta}-\boldmath{y})\\
&= \frac{1}{2}(\boldmath{\theta}^\mathrm{T}\boldmath{X}^\mathrm{T}\boldmath{X}\boldmath{\theta}-\boldmath{y}^\mathrm{T}\boldmath{X}\boldmath{\theta}-\boldmath{\theta}^\mathrm{T}\boldmath{X}^\mathrm{T}\boldmath{y}+\boldmath{y}^\mathrm{T}\boldmath{y})
\end{aligned}
$$

## 求解学习

### 正规方程

为了得到学习的参数$\boldmath{\theta}$，我们首先可以直接求解

$$
\boldmath{\theta}^*=\mathop{\arg\min}\limits_{\theta}J(\boldmath{\theta})
$$

即求损失函数$J(\boldmath{\theta})$的最小值点，先对其求一阶导

$$
\begin{aligned}
\nabla_{\boldmath{\theta}}J(\boldmath{\theta}) &= \frac{\partial J(\boldmath{\theta})}{\partial\boldmath{\theta}}\\
&= \frac{1}{2}(2\boldmath{X}^\mathrm{T}\boldmath{X}\boldmath{\theta}-2\boldmath{X}^\mathrm{T}\boldmath{y})\\
&= \boldmath{X}^\mathrm{T}\boldmath{X}\boldmath{\theta}-\boldmath{X}^\mathrm{T}\boldmath{y}
\end{aligned}
$$

再有二阶导

$$
\frac{\partial\nabla_{\boldmath{\theta}}J(\boldmath{\theta})}{\partial\boldmath{\theta}} = \boldmath{X}^\mathrm{T}\boldmath{X}
$$

**关于$\boldmath{X}^\mathrm{T}\boldmath{X}$**

$\boldmath{H} = \boldmath{X}^\mathrm{T}\boldmath{X}$总是半正定的（PSD，positive semi-definite），因为对任意非零列向量$\boldmath{a}$，总有

$$
\boldmath{a}^\mathrm{T}\boldmath{H}\boldmath{a} = \boldmath{a}^\mathrm{T}\boldmath{X}^\mathrm{T}\boldmath{X}\boldmath{a} = \Vert\boldmath{X}\boldmath{a}\Vert_2^2 \geq 0
$$

且当$\boldmath{X}$列满秩时，$\boldmath{H}$为正定的（PD，pisitive definite）,故$\boldmath{H} = \boldmath{X}^\mathrm{T}\boldmath{X}$可逆

**回到函数**

由$J(\boldmath{\theta})$的二阶导，即 Hessian 矩阵可知，一般情况下，$\boldmath{X}$的行数远大于列数（样本数远多于特征数），这意味着$\boldmath{X}$是列满秩的，故$J(\boldmath{\theta})$的二阶导正定可逆

进一步我们知道，多元函数$J(\boldmath{\theta})$是凸函数

- 极值点便是最值点
- 极值点$\nabla_{\boldmath{\theta}}J(\boldmath{\theta}) = 0$是极小值点

故我们可以直接写出$\boldmath{\theta}^*$的解析解

$$
\begin{aligned}
\nabla_{\boldmath{\theta}}J(\boldmath{\theta}) &= \boldmath{X}^\mathrm{T}\boldmath{X}\boldmath{\theta}-\boldmath{X}^\mathrm{T}\boldmath{y} = 0\\
\boldmath{\theta}^*&=  (\boldmath{X}^\mathrm{T}\boldmath{X})^{-1}\boldmath{X}^\mathrm{T}\boldmath{y}
\end{aligned}
$$

**计算开销粗估**

$$
\boldmath{\theta}^* =  (\boldmath{X}^\mathrm{T}\boldmath{X})^{-1}\boldmath{X}^\mathrm{T}\boldmath{y}
$$

$\boldmath{X}^\mathrm{T}\boldmath{X}$：$O(p^2n)$，$\boldmath{X}^\mathrm{T}\boldmath{y}$：$O(p^3)$，合起来$O(p^2n+p^3)$

当$n >> p$时，前一步乘积开销更大

### 梯度下降

