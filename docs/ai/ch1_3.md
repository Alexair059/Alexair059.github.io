---
comments: true
---

# 线性回归

线性回归（linear regression）是应用最为广泛的回归模型，但其基本思想与形式早已超出了单纯的回归场景。因为其推导形式比较简单、经典，就拿来记录一下，顺带结合相关的优化学习与正则技巧一起了

## 分类
- 监督学习方法
- 非概率模型
- 参数化模型

## 基本形式

假设样本写作增广后的列向量，即

$$
\bm{x} = [1,x^{(1)},x^{(2)},···,x^{(p)}]^\mathrm{T}
$$

即第一个对应参数的偏置项，剩下的即是样本的特征项，线性回归的模型相当简洁

$$
y = \theta_0+\theta_1x^{(1)}+\theta_2x^{(2)}+···\theta_px^{(p)}
$$

即

$$
y = \bm{x}^\mathrm{T}\bm{\theta}
$$

将整个样本写作矩阵形式，有

$$
\begin{aligned}
\bm{X} &= [\bm{x_1},\bm{x_2},···,\bm{x_n}]^\mathrm{T}\\
\bm{y} &= [y_1,y_2,···,y_n]^\mathrm{T}\\
\bm{\theta} &= [\theta_0,\theta_1,···,\theta_p]^\mathrm{T}
\end{aligned}
$$

则

$$
\bm{y} = f(\bm{X};\bm{\theta}) = \bm{X}\bm{\theta}
$$

## 损失策略

采取 SSE（sum of squared error），就是常见的平方损失，该函数是凸函数，这一优秀的性质是正规方程求最优解的基础

$$
\begin{aligned}
J(\bm{\theta}) &= \frac{1}{2}\sum_{i=1}^{n}(\hat{y_i}-y_i)^2\\
% &= \frac{1}{2}\sum_{i=1}^{n}(\bm{x}_i^\mathrm{T}\bm{\theta}-y_i)^2\\
&= \frac{1}{2}(\hat{\bm{y}}-\bm{y})^\mathrm{T}(\hat{\bm{y}}-\bm{y})\\
&= \frac{1}{2}(\bm{X}\bm{\theta}-\bm{y})^\mathrm{T}(\bm{X}\bm{\theta}-\bm{y})\\
&= \frac{1}{2}(\bm{\theta}^\mathrm{T}\bm{X}^\mathrm{T}\bm{X}\bm{\theta}-\bm{y}^\mathrm{T}\bm{X}\bm{\theta}-\bm{\theta}^\mathrm{T}\bm{X}^\mathrm{T}\bm{y}+\bm{y}^\mathrm{T}\bm{y})
\end{aligned}
$$

## 求解学习

### 正规方程

为了得到学习的参数$\bm{\theta}$，我们首先可以直接求解

$$
\bm{\theta}^*=\mathop{\arg\min}\limits_{\theta}J(\bm{\theta})
$$

即求损失函数$J(\bm{\theta})$的最小值点，先对其求一阶导

$$
\begin{aligned}
\nabla_{\bm{\theta}}J(\bm{\theta}) &= \frac{\partial J(\bm{\theta})}{\partial\bm{\theta}}\\
&= \frac{1}{2}(2\bm{X}^\mathrm{T}\bm{X}\bm{\theta}-2\bm{X}^\mathrm{T}\bm{y})\\
&= \bm{X}^\mathrm{T}\bm{X}\bm{\theta}-\bm{X}^\mathrm{T}\bm{y}
\end{aligned}
$$

再有二阶导

$$
\frac{\partial\nabla_{\bm{\theta}}J(\bm{\theta})}{\partial\bm{\theta}} = \bm{X}^\mathrm{T}\bm{X}
$$

**关于$\bm{X}^\mathrm{T}\bm{X}$**

$\bm{H} = \bm{X}^\mathrm{T}\bm{X}$总是半正定的（PSD，positive semi-definite），因为对任意非零列向量$\bm{a}$，总有

$$
\bm{a}^\mathrm{T}\bm{H}\bm{a} = \bm{a}^\mathrm{T}\bm{X}^\mathrm{T}\bm{X}\bm{a} = \Vert\bm{X}\bm{a}\Vert_2^2 \geq 0
$$

且当$\bm{X}$列满秩时，$\bm{H}$为正定的（PD，pisitive definite）,故$\bm{H} = \bm{X}^\mathrm{T}\bm{X}$可逆

**回到函数**

由$J(\bm{\theta})$的二阶导，即 Hessian 矩阵可知，一般情况下，$\bm{X}$的行数远大于列数（样本数远多于特征数），这意味着$\bm{X}$是列满秩的，故$J(\bm{\theta})$的二阶导正定可逆

进一步我们知道，多元函数$J(\bm{\theta})$是凸函数

- 极值点便是最值点
- 极值点$\nabla_{\bm{\theta}}J(\bm{\theta}) = 0$是极小值点

故我们可以直接写出$\bm{\theta}^*$的解析解

$$
\begin{aligned}
\nabla_{\bm{\theta}}J(\bm{\theta}) &= \bm{X}^\mathrm{T}\bm{X}\bm{\theta}-\bm{X}^\mathrm{T}\bm{y} = 0\\
\bm{\theta}^*&=  (\bm{X}^\mathrm{T}\bm{X})^{-1}\bm{X}^\mathrm{T}\bm{y}
\end{aligned}
$$

**计算开销粗估**

$$
\bm{\theta}^* =  (\bm{X}^\mathrm{T}\bm{X})^{-1}\bm{X}^\mathrm{T}\bm{y}
$$

$\bm{X}^\mathrm{T}\bm{X}$：$O(p^2n)$，$\bm{X}^\mathrm{T}\bm{y}$：$O(p^3)$，合起来$O(p^2n+p^3)$

当$n >> p$时，前一步乘积开销更大

### 梯度下降

