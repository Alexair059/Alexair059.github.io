---
comments: true
---

# 线性回归

线性回归（linear regression）是应用最为广泛的回归模型，但其基本思想与形式早已超出了单纯的回归场景。因为其推导形式比较简单、经典，就拿来记录一下，顺带结合相关的优化学习与正则技巧一起了

## 分类
- 监督学习方法
- 非概率模型
- 参数化模型

## 基本形式

假设样本写作增广后的列向量，即

$$
\mathbf{x} = [1,x^{(1)},x^{(2)},···,x^{(p)}]^\mathrm{T}
$$

即第一个对应参数的偏置项，剩下的即是样本的特征项，线性回归的模型相当简洁

$$
y = \theta_0+\theta_1x^{(1)}+\theta_2x^{(2)}+···\theta_px^{(p)}
$$

即

$$
y = \mathbf{x}^\mathrm{T}\boldsymbol{\theta}
$$

将整个样本写作矩阵形式，有

$$
\begin{aligned}
\mathbf{X} &= [\mathbf{x_1},\mathbf{x_2},···,\mathbf{x_n}]^\mathrm{T}\\
\mathbf{y} &= [y_1,y_2,···,y_n]^\mathrm{T}\\
\boldsymbol{\theta} &= [\theta_0,\theta_1,···,\theta_p]^\mathrm{T}
\end{aligned}
$$

则

$$
\mathbf{y} = f(\mathbf{X};\boldsymbol{\theta}) = \mathbf{X}\boldsymbol{\theta}
$$

## 损失策略

采取 SSE（sum of squared error），就是常见的平方损失，该函数是凸函数，这一优秀的性质是正规方程求最优解的基础

$$
\begin{aligned}
J(\boldsymbol{\theta}) &= \frac{1}{2}\sum_{i=1}^{n}(\hat{y_i}-y_i)^2\\
% &= \frac{1}{2}\sum_{i=1}^{n}(\mathbf{x}_i^\mathrm{T}\boldsymbol{\theta}-y_i)^2\\
&= \frac{1}{2}(\hat{\mathbf{y}}-\mathbf{y})^\mathrm{T}(\hat{\mathbf{y}}-\mathbf{y})\\
&= \frac{1}{2}(\mathbf{X}\boldsymbol{\theta}-\mathbf{y})^\mathrm{T}(\mathbf{X}\boldsymbol{\theta}-\mathbf{y})\\
&= \frac{1}{2}(\boldsymbol{\theta}^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{X}\boldsymbol{\theta}-\mathbf{y}^\mathrm{T}\mathbf{X}\boldsymbol{\theta}-\boldsymbol{\theta}^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{y}+\mathbf{y}^\mathrm{T}\mathbf{y})
\end{aligned}
$$

## 求解学习

### 正规方程

为了得到学习的参数$\boldsymbol{\theta}$，我们首先可以直接求解

$$
\boldsymbol{\theta}^*=\mathop{\arg\min}\limits_{\theta}J(\boldsymbol{\theta})
$$

即求损失函数$J(\boldsymbol{\theta})$的最小值点，先对其求一阶导

$$
\begin{aligned}
\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}) &= \frac{\partial J(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}}\\
&= \frac{1}{2}(2\mathbf{X}^\mathrm{T}\mathbf{X}\boldsymbol{\theta}-2\mathbf{X}^\mathrm{T}\mathbf{y})\\
&= \mathbf{X}^\mathrm{T}\mathbf{X}\boldsymbol{\theta}-\mathbf{X}^\mathrm{T}\mathbf{y}
\end{aligned}
$$

再有二阶导

$$
\frac{\partial\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}} = \mathbf{X}^\mathrm{T}\mathbf{X}
$$

**关于$\mathbf{X}^\mathrm{T}\mathbf{X}$**

$\mathbf{H} = \mathbf{X}^\mathrm{T}\mathbf{X}$总是半正定的（PSD，positive semi-definite），因为对任意非零列向量$\mathbf{a}$，总有

$$
\mathbf{a}^\mathrm{T}\mathbf{H}\mathbf{a} = \mathbf{a}^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{X}\mathbf{a} = \Vert\mathbf{X}\mathbf{a}\Vert_2^2 \geq 0
$$

且当$\mathbf{X}$列满秩时，$\mathbf{H}$为正定的（PD，pisitive definite）,故$\mathbf{H} = \mathbf{X}^\mathrm{T}\mathbf{X}$可逆

**回到函数**

由$J(\boldsymbol{\theta})$的二阶导，即 Hessian 矩阵可知，一般情况下，$\mathbf{X}$的行数远大于列数（样本数远多于特征数），这意味着$\mathbf{X}$是列满秩的，故$J(\boldsymbol{\theta})$的二阶导正定可逆

进一步我们知道，多元函数$J(\boldsymbol{\theta})$是凸函数

- 极值点便是最值点
- 极值点$\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}) = 0$是极小值点

故我们可以直接写出$\boldsymbol{\theta}^*$的解析解

$$
\begin{aligned}
\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}) &= \mathbf{X}^\mathrm{T}\mathbf{X}\boldsymbol{\theta}-\mathbf{X}^\mathrm{T}\mathbf{y} = 0\\
\boldsymbol{\theta}^*&=  (\mathbf{X}^\mathrm{T}\mathbf{X})^{-1}\mathbf{X}^\mathrm{T}\mathbf{y}
\end{aligned}
$$

**计算开销粗估**

$$
\boldsymbol{\theta}^* =  (\mathbf{X}^\mathrm{T}\mathbf{X})^{-1}\mathbf{X}^\mathrm{T}\mathbf{y}
$$

$\mathbf{X}^\mathrm{T}\mathbf{X}$：$O(p^2n)$，$\mathbf{X}^\mathrm{T}\mathbf{y}$：$O(p^3)$，合起来$O(p^2n+p^3)$

当$n >> p$时，前一步乘积开销更大

### 梯度下降

